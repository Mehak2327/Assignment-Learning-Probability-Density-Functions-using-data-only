# -*- coding: utf-8 -*-
"""Learning Probability Density Functions using Data Only.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zcxcbHTkiuSQWTUplsvP-B22lRL8b_Fi
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from sklearn.neighbors import KernelDensity
from google.colab import files
uploaded = files.upload()

# -------------------------------
# Load Dataset
# -------------------------------
import io

df = pd.read_csv(io.BytesIO(uploaded[list(uploaded.keys())[0]]),
                 encoding='latin1',
                 low_memory=False)

x = df['no2'].dropna().values.astype(np.float32)

# -------------------------------
# Transformation (Roll No Based)
# -------------------------------

a_r = 1.5
b_r = 0.9

z = x + a_r * np.sin(b_r * x)
z = z.reshape(-1,1)

# Normalize for GAN
z_mean, z_std = z.mean(), z.std()
z_norm = (z - z_mean) / z_std

real_data = torch.tensor(z_norm)

# -------------------------------
# GAN Models
# -------------------------------

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1,32),
            nn.ReLU(),
            nn.Linear(32,32),
            nn.ReLU(),
            nn.Linear(32,1)
        )
    def forward(self, x):
        return self.net(x)

class Discriminator(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1,32),
            nn.ReLU(),
            nn.Linear(32,32),
            nn.ReLU(),
            nn.Linear(32,1),
            nn.Sigmoid()
        )
    def forward(self, x):
        return self.net(x)

G = Generator()
D = Discriminator()

criterion = nn.BCELoss()
opt_G = optim.Adam(G.parameters(), lr=0.001)
opt_D = optim.Adam(D.parameters(), lr=0.001)

# -------------------------------
# Training
# -------------------------------

epochs = 400
batch_size = 128

for epoch in range(epochs):

    idx = torch.randint(0, real_data.shape[0], (batch_size,))
    real_batch = real_data[idx]

    noise = torch.randn(batch_size,1)
    fake_batch = G(noise)

    loss_D = criterion(D(real_batch), torch.ones(batch_size,1)) + \
             criterion(D(fake_batch.detach()), torch.zeros(batch_size,1))

    opt_D.zero_grad()
    loss_D.backward()
    opt_D.step()

    noise = torch.randn(batch_size,1)
    fake_batch = G(noise)

    loss_G = criterion(D(fake_batch), torch.ones(batch_size,1))

    opt_G.zero_grad()
    loss_G.backward()
    opt_G.step()

# -------------------------------
# Generate Samples
# -------------------------------

with torch.no_grad():
    gen_samples = G(torch.randn(8000,1)).numpy()

gen_samples = gen_samples * z_std + z_mean

# -------------------------------
# KDE PDF Estimation
# -------------------------------

kde = KernelDensity(kernel='gaussian', bandwidth=0.4)
kde.fit(gen_samples)

z_plot = np.linspace(gen_samples.min(), gen_samples.max(), 1000).reshape(-1,1)
pdf = np.exp(kde.score_samples(z_plot))

# -------------------------------
# Plot PDF
# -------------------------------

plt.figure(figsize=(6,4))
plt.plot(z_plot, pdf)
plt.xlabel("z")
plt.ylabel("Estimated PDF p(z)")
plt.title("PDF learned using GAN")
plt.show()

